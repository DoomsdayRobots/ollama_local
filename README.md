# ollama_local
This is a way to locally run ollama in python this could easily be adapted to be used with ROS1 or ROS2.

The prerequisets are, ollama pre-installed on your device, any llm models you wish to use should also be also 
installed on your device. Lastly you will need the python module "ollama" installed. To do this you can use 
pip. In a terminal or command line type: pip install ollama. if that dosen't work, for 
python 2 try: python -m pip install ollama. for python 3 try: python3 -m pip install ollama.

There are some fun "personas" to try out. Have your favorate llm answer like an orc, or answer like 
a ditzy college girl. Have fun, try messing around with the code. It's easy. Have your llm answer any 
way you like by adding your own persona option, eg, answer like an old miner from the gold rush era.  
